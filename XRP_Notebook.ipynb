{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7475f6-54a6-407e-bb97-7658371c9a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#We allocatte the file to a variable\n",
    "data = pd.read_csv('XRP-USD.csv')\n",
    "print('import done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef4c5c6-6e38-4e56-a832-df4e99140bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 1 discovering the datasets\n",
    "print(\" \")\n",
    "print(\"------------------------------Step 1 Discovering-------------------------------\")\n",
    "#We display the dimension of the dataset\n",
    "print(\"------------------------------Dimensions Step-------------------------------\")\n",
    "print(\" \")\n",
    "print(\"Number of rows:\", data.shape[0])\n",
    "print(\"Number of columns:\", data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539eaf2-8d2a-44c6-bd61-fe5e6514b7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We display the types of each columns and different informations about those columns such as their type or if they are any null values in each columns in order to make desicions afterward.\n",
    "print(\" \")\n",
    "print(\"------------------------------Information Step-------------------------------\")\n",
    "print(\" \")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2808bd7a-ff9e-40de-96a1-9ec7b755406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We describe the dataset through statistics in order to make desicions afterward.\n",
    "print(\" \")\n",
    "print(\"------------------------------Statistic Step-------------------------------\")\n",
    "print(\" \")\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45eca91-9b27-4926-b821-5eecdb1b5fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the first 5 lines of the datasets to have an overview of the dateset with real value\n",
    "print(\" \")\n",
    "print(\"------------------------------Look First Line Step-------------------------------\")\n",
    "print(\" \")\n",
    "#First lines \n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f237643e-acf9-4808-bbb2-31551b0adb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the last 5 of the datasets to have an overview of the dateset with real value to see if their any change between those and the first values\n",
    "print(\" \")\n",
    "print(\"------------------------------Look Last Line Step-------------------------------\")\n",
    "print(\" \")\n",
    "#Last lines\n",
    "data.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fc8e75-3cde-468f-b48d-2ce917b11f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \")\n",
    "print(\"------------------------------Step 2 Data cleaning-------------------------------\")\n",
    "#We Check if they are missing values again\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8aae6f-2b6a-4889-9738-6e1738f80126",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We see that they are no N/A value in our datasets so we will go straight to the scaling and normilzing parts because their no need of removing any values\n",
    "#Before choosing the method to scale our data and normalize it, we are going to visualize the correlation between each variable of datasets by using this method.\n",
    "print(\" \")\n",
    "print(\"------------------------------Step 3 Data scaling-------------------------------\")\n",
    "import seaborn as sns\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32ebd5-6c0b-4dab-a4c0-f69a40cc671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will also display a lineplot of the closing price link to the date data because we will try to determine the futur price of the monney so it is a\n",
    "# very important data to figure it out\n",
    "sns.lineplot(data=data,x=\"Date\",y=\"Close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5135cff0-2768-4da7-8c8c-407672de79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To scale our data, we will drop the date column, because it's a text column so it is not mandatory and not possible to sclale it\n",
    "to_scale = data.drop(['Date'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5289137-b932-4b04-aabe-8232d9fbf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#As we can see with those graph all the scale are pretty much at the same scale except for the volume column wich are at an extremely litte scale\n",
    "#So we will scale those value between 0 and 1 in order to be able to work correctly\n",
    "\n",
    "#We choose to use the min max scaler to sclale the data because we want to keep the same distrubution between the values and we want to control \n",
    "#the min and max value\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))  # Set the desired range\n",
    "\n",
    "# Fit and transform the data with the scale data\n",
    "scaled_data = scaler.fit_transform(to_scale)\n",
    "\n",
    "scaled_data =  pd.DataFrame(scaled_data,columns=to_scale.columns)\n",
    "#We add the date column again to the data now that the scale step is finished\n",
    "scaled_data = pd.concat([data['Date'],scaled_data],axis=1)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102cc06-4d04-4aca-9e47-96181fc63448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We redisplay diagram of ou data\n",
    "#As we can see now all the values are at the same scale so we can work\n",
    "sns.pairplot(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63599e04-b5ba-42ce-8f51-d2a56c22c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data\n",
    "#Again we drop the date column to normalize the data wich are numbers\n",
    "to_normalize = scaled_data.drop(['Date'],axis=1)\n",
    "to_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb22ba0-4352-4bb4-bf0c-6041ca520d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the data are very volatile,he will do a normalisation on those date because as we can see on the diagram we display earlier they are many outliers\n",
    "\n",
    "#We've choose to use an L2 normalisation because in our case, having time series data, that are represented as the close price of each day \n",
    "#of the XRP token, L2 normalization is going to help us to capture the natural variance of the data without emphasizing extreme values.\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Initialize Normalizer for L2 normalization\n",
    "normalizer_L2 = Normalizer(norm='l2')  \n",
    "\n",
    "\n",
    "# Apply L2 normalization\n",
    "normalized_data_L2 = normalizer_L2.transform(to_normalize)\n",
    "normalized_data_L2 =  pd.DataFrame(normalized_data_L2,columns=to_normalize.columns)\n",
    "#We add the date column to the other normalized data\n",
    "normalized_data_L2 = pd.concat([data['Date'],normalized_data_L2],axis=1)\n",
    "#We display the data\n",
    "print(\"\\nL2 Normalized Data:\")\n",
    "normalized_data_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd8821-29eb-447f-bba7-e9195c472a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(normalized_data_L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5781f3f-5823-4b62-b108-304c0fe4d3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As we don't have any categorical values that we need to trasform into numerical value we will skip the encoding part. \n",
    "#That's mark the end of the cleaning process of our data in our point of view."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
